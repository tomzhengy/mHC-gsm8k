import gc
import json
import re
import time
from pathlib import Path

import pytest
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

from routing import MultiStreamDecoder, greedy_decode, apply_mixing, compute_stream_diagnostics


# ============================================================================
# Fixtures
# ============================================================================

@pytest.fixture(scope="module")
def device():
    """Get appropriate device."""
    return "cuda" if torch.cuda.is_available() else "cpu"


@pytest.fixture(scope="module")
def model_name():
    """Model to use for testing. Override with smaller model if needed."""
    return "Qwen/Qwen3-4B-Instruct-2507"


@pytest.fixture(scope="module")
def tokenizer(model_name):
    """Load tokenizer once for all tests."""
    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    return tok


@pytest.fixture(scope="module")
def base_model(model_name, device):
    """Load base model once for all tests."""
    dtype = torch.float16 if device == "cuda" else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=dtype,
        device_map=device,
        trust_remote_code=True,
    )
    model.eval()
    return model


@pytest.fixture(scope="module")
def wrapped_model(base_model, device):
    """Create wrapped model once for all tests."""
    wrapped = MultiStreamDecoder(
        base_model,
        n_streams=4,
        mixing_mode="row_stochastic",
        collect_diagnostics=True,
    )
    wrapped.freeze_base()
    wrapped.eval()
    
    # Move mixing module to same device as base model
    if device == "cuda":
        wrapped.mixing = wrapped.mixing.cuda()
    
    return wrapped


@pytest.fixture
def test_prompts():
    """Standard test prompts."""
    return [
        "The capital of France is",
        "Question: What is 2 + 2?\nAnswer:",
        "def fibonacci(n):\n    ",
        "Once upon a time, there was a",
        "The solution to the equation x^2 = 4 is",
    ]


@pytest.fixture
def gsm8k_samples():
    """Load a few GSM8K samples for behavioral testing."""
    data_path = Path("data/gsm8k_train.jsonl")
    if not data_path.exists():
        pytest.skip("GSM8K data not found. Run scripts/prepare_data.py first.")
    
    samples = []
    with open(data_path) as f:
        for i, line in enumerate(f):
            if i >= 100:  # Load first 100
                break
            samples.append(json.loads(line))
    return samples


# ============================================================================
# Test 1: No-op Equivalence (g=0 should match baseline)
# ============================================================================

class TestNoopEquivalence:
    """
    Verify wrapper doesn't change the model when mixing is off (g=0).
    
    Pass criteria:
    - Logits match (or extremely close) for next-token distribution
    - Generated text identical for greedy decode
    """
    
    def test_logits_match_single_prompt(self, base_model, wrapped_model, tokenizer, device):
        """Test that g=0 produces identical logits to baseline."""
        prompt = "The capital of France is"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            # Baseline
            base_out = base_model(**inputs)
            base_logits = base_out.logits
            
            # Wrapped with g=0
            wrapped_out = wrapped_model(**inputs, g=0.0)
            wrapped_logits = wrapped_out.logits
        
        # Compare
        diff = (base_logits - wrapped_logits).abs()
        mean_diff = diff.mean().item()
        max_diff = diff.max().item()
        
        assert mean_diff < 1e-4, f"Mean logit diff too large: {mean_diff}"
        assert max_diff < 1e-3, f"Max logit diff too large: {max_diff}"
    
    def test_logits_match_multiple_prompts(self, base_model, wrapped_model, tokenizer, device, test_prompts):
        """Test logit matching across multiple prompts."""
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            with torch.no_grad():
                base_logits = base_model(**inputs).logits
                wrapped_logits = wrapped_model(**inputs, g=0.0).logits
            
            diff = (base_logits - wrapped_logits).abs().mean().item()
            assert diff < 1e-4, f"Logit mismatch for prompt '{prompt[:30]}...': {diff}"
    
    def test_greedy_decode_identical(self, base_model, wrapped_model, tokenizer, device):
        """Test that greedy decoding produces identical output."""
        prompt = "Question: What is the square root of 16?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Set seeds for determinism
        torch.manual_seed(42)
        
        # Baseline greedy decode (manual)
        with torch.no_grad():
            base_generated = inputs.input_ids.clone()
            for _ in range(20):
                out = base_model(base_generated)
                next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
                base_generated = torch.cat([base_generated, next_token], dim=-1)
                if next_token.item() == tokenizer.eos_token_id:
                    break
        
        # Wrapped greedy decode with g=0
        torch.manual_seed(42)
        wrapped_generated = greedy_decode(
            wrapped_model,
            inputs.input_ids,
            max_new_tokens=20,
            g=0.0,
            eos_token_id=tokenizer.eos_token_id,
        )
        
        # Compare
        base_text = tokenizer.decode(base_generated[0], skip_special_tokens=True)
        wrapped_text = tokenizer.decode(wrapped_generated[0], skip_special_tokens=True)
        
        assert base_text == wrapped_text, (
            f"Greedy decode mismatch:\n"
            f"Base: {base_text}\n"
            f"Wrapped: {wrapped_text}"
        )
    
    def test_next_token_identical(self, base_model, wrapped_model, tokenizer, device, test_prompts):
        """Test that next token prediction is identical for all prompts."""
        mismatches = []
        
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            with torch.no_grad():
                base_next = base_model(**inputs).logits[0, -1].argmax().item()
                wrapped_next = wrapped_model(**inputs, g=0.0).logits[0, -1].argmax().item()
            
            if base_next != wrapped_next:
                mismatches.append((prompt, base_next, wrapped_next))
        
        assert len(mismatches) == 0, f"Next token mismatches: {mismatches}"


# ============================================================================
# Test 2: Control Works (changing g changes behavior)
# ============================================================================

class TestControlWorks:
    """
    Verify that g actually affects the forward pass.
    
    Pass criteria:
    - KL(logits_g0 || logits_g1) > 0 for most prompts
    - Output changes sometimes
    """
    
    def test_logits_differ_g0_vs_g1(self, wrapped_model, tokenizer, device, test_prompts):
        """Test that g=0 and g=1 produce different logits."""
        kl_divergences = []
        
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            with torch.no_grad():
                logits_g0 = wrapped_model(**inputs, g=0.0).logits[0, -1]
                logits_g1 = wrapped_model(**inputs, g=1.0).logits[0, -1]
            
            # Compute KL divergence
            p = F.softmax(logits_g0, dim=-1)
            q = F.softmax(logits_g1, dim=-1)
            kl = (p * (p.log() - q.log())).sum().item()
            kl_divergences.append(kl)
        
        # Most should have positive KL
        positive_kl = sum(1 for kl in kl_divergences if kl > 0.01)
        assert positive_kl >= len(test_prompts) // 2, (
            f"Too few prompts with positive KL: {positive_kl}/{len(test_prompts)}\n"
            f"KL values: {kl_divergences}"
        )
    
    def test_logits_vary_across_g_values(self, wrapped_model, tokenizer, device):
        """Test that logits vary across different g values."""
        prompt = "The answer to the math problem is"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        g_values = [0.0, 0.25, 0.5, 0.75, 1.0]
        logits_list = []
        
        with torch.no_grad():
            for g in g_values:
                logits = wrapped_model(**inputs, g=g).logits[0, -1]
                logits_list.append(logits)
        
        # Compute pairwise differences
        diffs = []
        for i in range(len(logits_list) - 1):
            diff = (logits_list[i] - logits_list[i + 1]).abs().mean().item()
            diffs.append(diff)
        
        # At least some adjacent pairs should differ
        significant_diffs = sum(1 for d in diffs if d > 0.1)
        assert significant_diffs >= 2, (
            f"Not enough variation across g values. Diffs: {diffs}"
        )
    
    def test_generation_changes_with_g(self, wrapped_model, tokenizer, device):
        """Test that generated text can change with different g."""
        prompt = "Question: What is 5 + 7?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        generations = {}
        for g in [0.0, 0.5, 1.0]:
            output_ids = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=30,
                g=g,
                eos_token_id=tokenizer.eos_token_id,
            )
            text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            generations[g] = text[len(prompt):]
        
        # At least one pair should differ
        unique_generations = len(set(generations.values()))
        # Note: It's OK if they're all the same (model might be robust)
        # but we should log this
        print(f"\nGenerations at different g:\n{generations}")
        
        # This is a soft assertion - we just want to see variety exists
        assert True, "Generation test completed (check output manually)"


# ============================================================================
# Test 3: Main Stream Influenced by Other Streams
# ============================================================================

class TestStreamInfluence:
    """
    Prove that streams 1..n actually contribute to the output.
    
    Pass criteria:
    - With g=0: perturbing non-main streams doesn't change output
    - With g=1: perturbing non-main streams changes output
    """
    
    def test_perturbation_effect(self, wrapped_model, tokenizer, device):
        """Test that perturbing non-main streams affects output only when g>0."""
        prompt = "The capital of"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Get embeddings
        with torch.no_grad():
            embeds = wrapped_model._embed(inputs.input_ids)
        
        # Initialize streams
        n_streams = wrapped_model.n_streams
        streams = embeds.unsqueeze(0).expand(n_streams, -1, -1, -1).clone()
        
        # Perturb stream 1 (not main stream 0)
        perturbation = torch.randn_like(streams[1]) * 0.5
        streams_perturbed = streams.clone()
        streams_perturbed[1] = streams[1] + perturbation
        
        # Get mixing matrix
        M = wrapped_model.mixing()
        
        # Apply mixing with g=0 (identity)
        streams_g0 = apply_mixing(streams, M, 0.0)
        streams_g0_perturbed = apply_mixing(streams_perturbed, M, 0.0)
        
        # Apply mixing with g=1 (full mixing)
        streams_g1 = apply_mixing(streams, M, 1.0)
        streams_g1_perturbed = apply_mixing(streams_perturbed, M, 1.0)
        
        # Check main stream (index 0)
        diff_g0 = (streams_g0[0] - streams_g0_perturbed[0]).abs().mean().item()
        diff_g1 = (streams_g1[0] - streams_g1_perturbed[0]).abs().mean().item()
        
        print(f"\nMain stream diff with g=0: {diff_g0:.6f}")
        print(f"Main stream diff with g=1: {diff_g1:.6f}")
        
        # With g=0, main stream should be unaffected
        assert diff_g0 < 1e-5, f"g=0 should not propagate perturbation: {diff_g0}"
        
        # With g=1, main stream should be affected
        assert diff_g1 > 0.01, f"g=1 should propagate perturbation: {diff_g1}"
    
    def test_full_forward_with_perturbation(self, wrapped_model, tokenizer, device):
        """Test full forward pass with stream perturbation hook."""
        prompt = "The capital of France is"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # This is a more complex test - we'll compare outputs
        # by running forward twice and checking difference
        
        with torch.no_grad():
            # Normal forward
            out_normal_g0 = wrapped_model(**inputs, g=0.0).logits
            out_normal_g1 = wrapped_model(**inputs, g=1.0).logits
        
        # The key insight: if we could inject perturbation mid-forward,
        # g=0 should be unaffected, g=1 should change
        # Since we can't easily hook, we verify via the mixing math test above
        
        assert True, "Full forward test completed"


# ============================================================================
# Test 4: Mixing Math is Correct
# ============================================================================

class TestMixingMath:
    """
    Verify the mixing operator behaves correctly.
    
    Pass criteria:
    - With g=0: streams remain mostly separate
    - With g>0: streams become more similar
    - No explosions in norms
    """
    
    def test_stream_separation_g0(self, wrapped_model, tokenizer, device):
        """Test that g=0 keeps streams separate."""
        prompt = "Hello world"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            out = wrapped_model(**inputs, g=0.0)
        
        diag = out.stream_diagnostics
        assert diag is not None, "Diagnostics not collected"
        
        # With g=0, streams should maintain diversity
        # (they start identical but only main stream changes)
        print(f"\nStream diagnostics (g=0): {diag}")
        
        # Norm ratio should be reasonable
        assert diag["stream_norm_ratio"] < 100, f"Norm ratio exploded: {diag['stream_norm_ratio']}"
    
    def test_stream_convergence_g1(self, wrapped_model, tokenizer, device):
        """Test that g=1 causes streams to become more similar."""
        prompt = "Hello world"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            out_g0 = wrapped_model(**inputs, g=0.0)
            out_g1 = wrapped_model(**inputs, g=1.0)
        
        diag_g0 = out_g0.stream_diagnostics
        diag_g1 = out_g1.stream_diagnostics
        
        print(f"\nStream diversity g=0: {diag_g0['stream_diversity']:.4f}")
        print(f"Stream diversity g=1: {diag_g1['stream_diversity']:.4f}")
        
        # With g=1, streams should mix and potentially have lower diversity
        # Note: This depends on the mixing matrix structure
        assert diag_g1 is not None
    
    def test_mixing_matrix_properties(self, wrapped_model):
        """Test that mixing matrix has expected properties."""
        diag = wrapped_model.mixing.get_diagnostics()
        
        print(f"\nMixing matrix diagnostics: {diag}")
        
        # Row sums should be ~1 (row stochastic)
        assert diag["mix_row_sums_std"] < 0.01, "Row sums not close to 1"
        
        # Diagonal should be dominant (near-identity init)
        assert diag["mix_diagonal_mean"] > 0.8, "Diagonal not dominant enough"
        
        # Off-diagonal should be small
        assert diag["mix_off_diagonal_max"] < 0.2, "Off-diagonal too large"
    
    def test_no_norm_explosion(self, wrapped_model, tokenizer, device, test_prompts):
        """Test that norms don't explode across different inputs."""
        norm_ratios = []
        
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            for g in [0.0, 0.5, 1.0]:
                with torch.no_grad():
                    out = wrapped_model(**inputs, g=g)
                
                if out.stream_diagnostics:
                    norm_ratios.append(out.stream_diagnostics["stream_norm_ratio"])
        
        max_ratio = max(norm_ratios)
        assert max_ratio < 100, f"Norm ratio too high: {max_ratio}"
        print(f"\nMax norm ratio across tests: {max_ratio:.2f}")


# ============================================================================
# Test 5: Gradients / Training Sanity
# ============================================================================

class TestGradients:
    """
    Verify gradient flow is correct.
    
    Pass criteria:
    - Base LM params have no grads (frozen)
    - Only controller/mix params have grads if allowed
    """
    
    def test_base_model_frozen(self, base_model, wrapped_model, tokenizer, device):
        """Test that base model params don't receive gradients."""
        prompt = "Test gradient flow"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Forward pass
        out = wrapped_model(**inputs, g=0.5)
        
        # Create dummy loss and backward
        loss = out.logits.mean()
        loss.backward()
        
        # Check base model has no grads
        base_grads = []
        for name, param in wrapped_model.base.named_parameters():
            if param.grad is not None:
                base_grads.append(name)
        
        assert len(base_grads) == 0, f"Base model has gradients: {base_grads[:5]}..."
        
        # Clean up
        wrapped_model.zero_grad()
    
    def test_mixing_params_have_grads(self, wrapped_model, tokenizer, device):
        """Test that mixing params receive gradients when not frozen."""
        # Temporarily unfreeze mixing
        wrapped_model.mixing.requires_grad_(True)
        
        prompt = "Test gradient flow"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        out = wrapped_model(**inputs, g=0.5)
        loss = out.logits.mean()
        loss.backward()
        
        # Check mixing params have grads
        assert wrapped_model.mixing.logits.grad is not None, "Mixing logits have no gradient"
        
        # Clean up
        wrapped_model.zero_grad()
        wrapped_model.mixing.requires_grad_(False)


# ============================================================================
# Test 6: Forward Speed and Determinism
# ============================================================================

class TestSpeedAndDeterminism:
    """
    Verify environment is stable for RL rollouts.
    
    Pass criteria:
    - Time per rollout stable
    - Deterministic decode with greedy
    - No memory leak across episodes
    """
    
    def test_determinism(self, wrapped_model, tokenizer, device):
        """Test that greedy decode is deterministic."""
        prompt = "The answer is"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        generations = []
        for _ in range(3):
            torch.manual_seed(42)
            output_ids = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=20,
                g=0.5,
                eos_token_id=tokenizer.eos_token_id,
            )
            text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            generations.append(text)
        
        assert len(set(generations)) == 1, f"Non-deterministic: {generations}"
    
    def test_forward_speed(self, wrapped_model, tokenizer, device):
        """Test forward pass speed is reasonable."""
        prompt = "Question: What is 2 + 2?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Warmup
        with torch.no_grad():
            for _ in range(3):
                _ = wrapped_model(**inputs, g=0.5)
        
        # Time multiple forwards
        n_runs = 10
        times = []
        
        with torch.no_grad():
            for _ in range(n_runs):
                start = time.perf_counter()
                _ = wrapped_model(**inputs, g=0.5)
                times.append(time.perf_counter() - start)
        
        avg_time = sum(times) / len(times)
        std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
        
        print(f"\nForward time: {avg_time*1000:.2f} Â± {std_time*1000:.2f} ms")
        
        # Should be reasonably fast (< 1s on CPU, much less on GPU)
        assert avg_time < 5.0, f"Forward too slow: {avg_time:.2f}s"
    
    def test_generation_speed(self, wrapped_model, tokenizer, device):
        """Test generation speed."""
        prompt = "Question: What is 2 + 2?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Warmup
        for _ in range(2):
            _ = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=20,
                g=0.5,
            )
        
        # Time
        n_runs = 5
        times = []
        
        for _ in range(n_runs):
            start = time.perf_counter()
            _ = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=50,
                g=0.5,
            )
            times.append(time.perf_counter() - start)
        
        avg_time = sum(times) / len(times)
        print(f"\nGeneration time (50 tokens): {avg_time:.2f}s")
        
        # Should complete in reasonable time
        assert avg_time < 60.0, f"Generation too slow: {avg_time:.2f}s"
    
    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_memory_stability(self, wrapped_model, tokenizer, device):
        """Test that memory doesn't leak across episodes."""
        prompt = "Question: What is 2 + 2?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Initial memory
        torch.cuda.empty_cache()
        gc.collect()
        initial_mem = torch.cuda.memory_allocated()
        
        # Run many episodes
        for i in range(50):
            _ = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=30,
                g=float(i % 5) / 4,  # Vary g
            )
            
            if i % 10 == 0:
                gc.collect()
                torch.cuda.empty_cache()
        
        # Final memory
        gc.collect()
        torch.cuda.empty_cache()
        final_mem = torch.cuda.memory_allocated()
        
        mem_increase = (final_mem - initial_mem) / 1024 / 1024  # MB
        print(f"\nMemory increase after 50 episodes: {mem_increase:.2f} MB")
        
        # Should not leak significantly
        assert mem_increase < 100, f"Memory leak detected: {mem_increase:.2f} MB"


# ============================================================================
# Test 7: Behavioral Validation on Task Slice
# ============================================================================

class TestBehavioralValidation:
    """
    Show routing changes accuracy in a measurable way.
    
    Pass criteria:
    - Accuracy differs across g values
    """
    
    @staticmethod
    def extract_answer(text: str) -> str | None:
        """Extract numeric answer from generated text."""
        # Look for patterns like "= 42", "is 42", "answer is 42"
        patterns = [
            r"=\s*(-?\d+(?:,\d+)*(?:\.\d+)?)",
            r"(?:is|equals?|answer)\s*[:=]?\s*(-?\d+(?:,\d+)*(?:\.\d+)?)",
            r"\$(-?\d+(?:,\d+)*(?:\.\d+)?)\$",
            r"(-?\d+(?:,\d+)*(?:\.\d+)?)\s*$",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).replace(",", "")
        
        return None
    
    def test_accuracy_varies_with_g(self, wrapped_model, tokenizer, device, gsm8k_samples):
        """Test that accuracy varies with different g values."""
        # Use first 50 samples for speed
        samples = gsm8k_samples[:50]
        
        results = {g: {"correct": 0, "total": 0} for g in [0.0, 0.5, 1.0]}
        
        for sample in samples:
            question = sample["question"]
            expected = sample["final_answer"]
            
            prompt = f"Question: {question}\nAnswer: Let me solve this step by step."
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
            
            for g in results.keys():
                output_ids = greedy_decode(
                    wrapped_model,
                    inputs.input_ids,
                    max_new_tokens=150,
                    g=g,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                predicted = self.extract_answer(generated)
                
                results[g]["total"] += 1
                if predicted == expected:
                    results[g]["correct"] += 1
        
        # Print results
        print("\n" + "=" * 50)
        print("Accuracy by g value:")
        print("=" * 50)
        for g, stats in results.items():
            acc = stats["correct"] / stats["total"] * 100
            print(f"g={g}: {acc:.1f}% ({stats['correct']}/{stats['total']})")
        
        # We don't require accuracy to differ (model might be robust)
        # but we log for analysis
        accuracies = [r["correct"] / r["total"] for r in results.values()]
        print(f"\nAccuracy range: {min(accuracies)*100:.1f}% - {max(accuracies)*100:.1f}%")


# ============================================================================
# Test 8: RL Learnability Smoke Test
# ============================================================================

class TestRLLearnability:
    """
    Verify RL can potentially learn from this setup.
    
    This is a minimal smoke test - actual RL training is done separately.
    """
    
    def test_reward_signal_exists(self, wrapped_model, tokenizer, device, gsm8k_samples):
        """Test that different g values can produce different rewards."""
        # Take 10 samples
        samples = gsm8k_samples[:10]
        
        rewards_by_g = {g: [] for g in [0.0, 0.25, 0.5, 0.75, 1.0]}
        
        for sample in samples:
            question = sample["question"]
            expected = sample["final_answer"]
            
            prompt = f"Question: {question}\nAnswer:"
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256).to(device)
            
            for g in rewards_by_g.keys():
                output_ids = greedy_decode(
                    wrapped_model,
                    inputs.input_ids,
                    max_new_tokens=100,
                    g=g,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                predicted = TestBehavioralValidation.extract_answer(generated)
                
                reward = 1.0 if predicted == expected else 0.0
                rewards_by_g[g].append(reward)
        
        # Print reward distribution
        print("\n" + "=" * 50)
        print("Reward distribution by g:")
        print("=" * 50)
        for g, rewards in rewards_by_g.items():
            avg = sum(rewards) / len(rewards)
            print(f"g={g}: avg_reward={avg:.2f}")
        
        # Check that at least some episodes get reward
        total_rewards = sum(sum(r) for r in rewards_by_g.values())
        assert total_rewards > 0, "No correct answers - reward signal may be too sparse"
    
    def test_action_space_coverage(self, wrapped_model, tokenizer, device):
        """Test that all gate values produce valid outputs."""
        prompt = "Question: What is 2 + 2?\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        gate_values = [0.0, 0.25, 0.5, 0.75, 1.0]
        
        for g in gate_values:
            # Should not raise
            output_ids = greedy_decode(
                wrapped_model,
                inputs.input_ids,
                max_new_tokens=20,
                g=g,
            )
            
            # Should produce valid text
            text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            assert len(text) > len("Question: What is 2 + 2?\nAnswer:"), f"No generation at g={g}"
        
        print(f"\nAll {len(gate_values)} gate values produce valid outputs")


# ============================================================================
# Run configuration
# ============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])

